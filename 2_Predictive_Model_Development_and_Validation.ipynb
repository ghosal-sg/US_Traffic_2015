{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noRlVlcBTMdn"
   },
   "outputs": [],
   "source": [
    "\"\"\"Importing Necessary Libraries\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from hyperopt import Trials, STATUS_OK, tpe, hp, fmin\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4uGay-imTMdu"
   },
   "outputs": [],
   "source": [
    "\"\"\"Importing already exported cleaned parquet file of the US_Traffic_2015 dataset\"\"\"\n",
    "\n",
    "US_Traffic_2015=pd.read_parquet('.\\\\Data\\\\US_Traffic_2015.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the pool of independent variables available we can develop a predictive model to know apriori the volume of traffic in a particular day, in a particular area, in the jurisdiction of a particular station id, direction of travel and type of road. With the predictions available, Traffic Control can make sufficient planning for the ease of passage of vehicles\n",
    "\n",
    "## Since we are predicting a continuous variable (Volume of Traffic), hence must to use a Regressor.\n",
    "\n",
    "## There are lots of options among Regressors, namely Linear Regression, Decision Tree Regressor, Random Forest Regressor, Gradient Boosting Regressor or XGBoost Regressor.\n",
    "\n",
    "## From the Bi-Variate analysis we observed that most of the independent variables are having a non-linear pattern with the target variables. Hence to develop a parametric model like Linear Regression, we have to perform lots of transformations to satisfy the assumptions.\n",
    "\n",
    "## Under such circumstances usage of Machine Learning is a better idea. Among the Machine Learning algorithms XGBoost is the most popular one across the industry for its options of regularisation (which controls overfit) and parallel processing (faster model trainings). XGBoost algorithm also supports GPU acceleration, and hence for the above advantages the XGBoost Regressor was chosen for development of the predictive model.\n",
    "\n",
    "## Since we have created 3 target variables namely, Traffic volumes of Non-Business Hours, Night-Time and Business Hours, we have chosen only one Target Variable for the predictive model development. Out of the 3, predicting traffic volumes during business hours (when there are a greater number of vehicles), will be more useful for practical usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Identifying Objective or Target Variables\"\"\"\n",
    "\n",
    "Target_Variables=[var for var in US_Traffic_2015.columns.tolist() if var.startswith('Traffic_Volume_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Retaining the Target Variable containing the information of Traffic Volume in Business Hours, and dropping the rest\"\"\"\n",
    "\n",
    "Target_Variable=Target_Variables[-1]\n",
    "Independent_Variables=list(set(US_Traffic_2015.columns.tolist())-set(Target_Variables))\n",
    "Variables_to_Drop=Target_Variables[:2]\n",
    "US_Traffic_2015.drop(Variables_to_Drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The entire dataset was split into 2 parts: a ~70% random sample for training and the remaining ~30% for In-Time Validation, hereafter will be called as Test.\n",
    "\n",
    "## Under circumstances where there is absence of Out of Time dataset for model performance evaluation a K-fold cross validation is an industry standard practice.\n",
    "\n",
    "## Since we have chosen XGBoost Regressor, we have to perform hyperparameter tuning to identify the most optimal model in order to attain stability and best performance in our Test dataset. For hyperparameter tuning there are multiple options, like Random Search, Manual Grid Search or Bayesian Optimisation to choose from. Bayesian Optimisation was chosen as the preferred method of hyperparameter tuning for its efficiency in moving towards the convergence with lesser number of experiments.\n",
    "\n",
    "## Hyperparameter tuning is itself an expensive process and using K-Fold cross validation on top of it makes it even more complex, - hence K-Fold cross validation was avoided. Such an experiment requires cloud compute and can't be processed in a personal laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting the dataset into Training and Hold Out sets with a ratio of 70:30\"\"\"\n",
    "\n",
    "X_Train, X_Test, y_Train, y_Test=train_test_split(US_Traffic_2015[Independent_Variables], US_Traffic_2015[Target_Variable], test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Converting the Train and Test datasets into XGBoost DMatrix\"\"\"\n",
    "\n",
    "dtrain=xgb.DMatrix(data=X_Train, label=y_Train)\n",
    "dtest=xgb.DMatrix(data=X_Test, label=y_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have created a custom objective function for the Bayesian approach of Hyperparameter tuning:\n",
    "\n",
    "## We want to maximise the Test dataset performance (here we have used adjusted R Squared as our evaluation metric), also we want to minimise overfit (Here we have defined overfit as the relative difference of adjusted R Squared between the Train dataset and the Test dataset. \n",
    "\n",
    "## In order to achieve this 2-fold optimisation here we have introduced a concept called Ideal Model. We have defined Ideal model as the one having ideal adjusted R Squared which is 1, and ideal overfit, which is 0 (No drop of Test performance from Train).\n",
    "\n",
    "## In a 2D Cartesian system (Test Performance vs Overfit) the ideal model can be marked with the point (1,0). For any given hyperparameter we can plot the respective Test Performance and Overfit in the cartesian plane. The best model will be the one having minimum Euclidean Distance from the Ideal Model\n",
    "\n",
    "## For all practical purposes we have limited the total number of HyperOpt trials to 100, and utilised GPU acceleration using the GeForce Gaming GPU available in the laptop, which is definitely faster than a Intel Dual Core CPU when parallel processing is taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNot5gLtTMdr"
   },
   "outputs": [],
   "source": [
    "\"\"\"Creating Helper Functions to run XGBoost using HyperOpt. Usage of each function has been explained below\"\"\"\n",
    "\n",
    "\"\"\"Creating a function for computing Adjusted R Squared, which will be used as one of our model evaluation metrics\"\"\"\n",
    "\n",
    "def Adjusted_R_Squared(Actuals, Predicted, nrows, ncols):\n",
    "    df=pd.DataFrame({'Actuals':Actuals,'Predicted':Predicted})\n",
    "    R_squared=1-(sum((df['Actuals']-df['Predicted'])**2)/sum((df['Actuals']-np.mean(df['Actuals']))**2))\n",
    "    adj_R_squared=1-(1-R_squared)*((nrows-1)/(nrows-ncols-1))\n",
    "    return(adj_R_squared)\n",
    "\n",
    "\"\"\"The HyperOpt function with custom objective, which takes care of maximisation of Hold Out Performance and minimisation of\n",
    "    overfit\"\"\"\n",
    "\n",
    "def HyperOpt_Objective(space):\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "    \n",
    "    params={\n",
    "        \n",
    "        \"learning_rate\":space['learning_rate'], \\\n",
    "        \"max_depth\":int(space['max_depth']), \\\n",
    "        \"gamma\":space['gamma'], \\\n",
    "        \"reg_lambda\":space['reg_lambda'], \\\n",
    "        \"reg_alpha\":space['reg_alpha'], \\\n",
    "        \"subsample\":space['subsample'], \\\n",
    "        \"colsample_bytree\":space['colsample_bytree'], \\\n",
    "        \"objective\":'reg:squarederror', \\\n",
    "        \"n_jobs\":-1, \\\n",
    "        \"tree_method\":'gpu_hist', \\\n",
    "        \"gpu_id\":0, \\\n",
    "        \"seed\":12345\n",
    "    }\n",
    "    \n",
    "    xgb_model=xgb.XGBRegressor(**params)\n",
    "    xgb_model=xgb.train(params=xgb_model.get_xgb_params(), dtrain=dtrain, num_boost_round=5000, evals=[(dtrain, 'train'), (dtest, 'test')], early_stopping_rounds=10, verbose_eval=False)\n",
    "    \n",
    "    Train_Adjusted_R_Squared=Adjusted_R_Squared(y_Train.tolist(), xgb_model.predict(dtrain).tolist(), X_Train.shape[0], X_Train.shape[1])\n",
    "    Test_Adjusted_R_Squared=Adjusted_R_Squared(y_Test.tolist(), xgb_model.predict(dtest).tolist(), X_Test.shape[0], X_Test.shape[1])\n",
    "    \n",
    "    if Train_Adjusted_R_Squared>Test_Adjusted_R_Squared:\n",
    "        Euclidean_Distance_from_Ideal=np.sqrt(((1-Test_Adjusted_R_Squared)**2+(0-((Train_Adjusted_R_Squared-Test_Adjusted_R_Squared)/Train_Adjusted_R_Squared))**2))\n",
    "    else:\n",
    "        Euclidean_Distance_from_Ideal=np.sqrt(((1-Test_Adjusted_R_Squared)**2))\n",
    "\n",
    "    return{'loss': Euclidean_Distance_from_Ideal, 'status': STATUS_OK, 'num_boost_round': xgb_model.best_ntree_limit, 'Train_Adjusted_R_Squared':Train_Adjusted_R_Squared, 'Test_Adjusted_R_Squared':Test_Adjusted_R_Squared}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [11:20:58<00:00, 408.58s/trial, best loss: 0.05151237610261228]\n",
      "Best model:  {'colsample_bytree': 0.5, 'gamma': 7.0, 'learning_rate': 0.04, 'max_depth': 9.0, 'reg_alpha': 0.0, 'reg_lambda': 7.0, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Running XGBoost using HyperOpt with custom objective function\"\"\"\n",
    "\n",
    "space = {\n",
    "            'learning_rate': hp.quniform('learning_rate', 0.01, 0.1, 0.01),\n",
    "            'max_depth': hp.quniform('max_depth', 1,10,1),\n",
    "            'gamma': hp.quniform('gamma', 0,10,1),\n",
    "            'reg_lambda': hp.quniform('reg_lambda', 0,10,1),\n",
    "            'reg_alpha': hp.quniform('reg_alpha', 0,10,1),\n",
    "            'subsample': hp.quniform('subsample', 0.1,1,0.1),\n",
    "            'colsample_bytree': hp.quniform('colsample_bytree', 0.1,1,0.1)\n",
    "        }\n",
    "\n",
    "trials=Trials()\n",
    "best=fmin(fn=HyperOpt_Objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "print('Best model: ', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extracting the HyperOpt Log for conversion into a pandas dataframe\"\"\"\n",
    "\n",
    "HyperOpt_Log=pd.DataFrame({})\n",
    "for i in range(len(trials.trials)):\n",
    "    Trial_Results=pd.DataFrame(trials.trials[i]['misc']['vals'])\n",
    "    Trial_Results.insert(0,'Step',[trials.trials[i]['misc']['tid']])\n",
    "    Trial_Results['num_boost_round']=[trials.trials[i]['result']['num_boost_round']]\n",
    "    Trial_Results['loss']=[trials.trials[i]['result']['loss']]\n",
    "    Trial_Results['Train_Adjusted_R_Squared']=[trials.trials[i]['result']['Train_Adjusted_R_Squared']]\n",
    "    Trial_Results['Test_Adjusted_R_Squared']=[trials.trials[i]['result']['Test_Adjusted_R_Squared']]\n",
    "    HyperOpt_Log=HyperOpt_Log.append(Trial_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Exporting the HyperOpt Log\"\"\"\n",
    "\n",
    "writer=pd.ExcelWriter('.\\Output\\HyperOpt_Log.xlsx', engine='xlsxwriter')\n",
    "HyperOpt_Log.to_excel(writer, sheet_name='HyperOpt_Log', index=False)\n",
    "for index, var in enumerate(HyperOpt_Log):\n",
    "    max_length=max(HyperOpt_Log[var].astype(str).map(len).max(), len(var))+1\n",
    "    writer.sheets['HyperOpt_Log'].set_column(index, index, max_length)    \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training the model with the best hyperparameter as per our custom objective function and HyperOpt\"\"\"\n",
    "\n",
    "HyperOpt_Log=HyperOpt_Log.sort_values('loss').reset_index(drop=True)\n",
    "\n",
    "params={\n",
    "\n",
    "    \"learning_rate\":HyperOpt_Log.iloc[:1,1:9].to_dict()['learning_rate'][0], \\\n",
    "    \"max_depth\":int(HyperOpt_Log.iloc[:1,1:9].to_dict()['max_depth'][0]), \\\n",
    "    \"gamma\":HyperOpt_Log.iloc[:1,1:9].to_dict()['gamma'][0], \\\n",
    "    \"reg_lambda\":HyperOpt_Log.iloc[:1,1:9].to_dict()['reg_lambda'][0], \\\n",
    "    \"reg_alpha\":HyperOpt_Log.iloc[:1,1:9].to_dict()['reg_alpha'][0], \\\n",
    "    \"subsample\":HyperOpt_Log.iloc[:1,1:9].to_dict()['subsample'][0], \\\n",
    "    \"colsample_bytree\":HyperOpt_Log.iloc[:1,1:9].to_dict()['colsample_bytree'][0], \\\n",
    "    \"objective\":'reg:squarederror', \\\n",
    "    \"n_jobs\":-1, \\\n",
    "    \"tree_method\":'gpu_hist', \\\n",
    "    \"gpu_id\":0, \\\n",
    "    \"seed\":12345\n",
    "}\n",
    "\n",
    "xgb_model=xgb.XGBRegressor(**params)\n",
    "xgb_model=xgb.train(params=xgb_model.get_xgb_params(), dtrain=dtrain, num_boost_round=int(HyperOpt_Log.iloc[:1,1:9].to_dict()['num_boost_round'][0]), evals=[(dtrain, 'train'), (dtest, 'test')], early_stopping_rounds=10, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the best model chosen using the HyperOpt search, the training process was executed once more to have the final model with the best chosen hyperparameter combination.\n",
    "\n",
    "## The Adjusted R Squared of the Train and Test datasets along with the Normalised Root Mean Squared Errors have been presented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R Squared of Train dataset: 0.9436536321561113\n",
      "Adjusted R Squared of Test dataset: 0.948497356140137\n",
      "\n",
      "\n",
      "NRMSE of Train dataset: 0.3604653615246144\n",
      "NRMSE of Test dataset: 0.34062225149390435\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluating the Train and Test performances based on the best model selection\"\"\"\n",
    "\n",
    "print(\"Adjusted R Squared of Train dataset: \"+str(Adjusted_R_Squared(y_Train.tolist(), xgb_model.predict(dtrain).tolist(), X_Train.shape[0], X_Train.shape[1])))\n",
    "print(\"Adjusted R Squared of Test dataset: \"+str(Adjusted_R_Squared(y_Test.tolist(), xgb_model.predict(dtest).tolist(), X_Test.shape[0], X_Test.shape[1])))\n",
    "print(\"\\n\")\n",
    "print(\"NRMSE of Train dataset: \"+str(mean_squared_error(y_Train.tolist(), xgb_model.predict(dtrain).tolist(), squared=False)/y_Train.mean()))\n",
    "print(\"NRMSE of Test dataset: \"+str(mean_squared_error(y_Test.tolist(), xgb_model.predict(dtest).tolist(), squared=False)/y_Test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most significant variables of the model are also printed below, and we see that significant value has been generated by means of the feature engineering. The Weather data along with the Holiday information were utilised multiple times for creating the tree splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tavg</td>\n",
       "      <td>60316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>59485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>month_of_data</td>\n",
       "      <td>56036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day_of_data</td>\n",
       "      <td>54591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>station_location_index</td>\n",
       "      <td>49466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lrs_location_point</td>\n",
       "      <td>38475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>posted_signed_route_number_index</td>\n",
       "      <td>36913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fips_county_code</td>\n",
       "      <td>36597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lane_of_travel</td>\n",
       "      <td>33319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>direction_of_travel</td>\n",
       "      <td>32968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>year_station_established</td>\n",
       "      <td>29943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lrs_identification_index</td>\n",
       "      <td>27378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Weekend_Flag</td>\n",
       "      <td>25282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>direction_of_travel_name_index</td>\n",
       "      <td>23718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hpms_sample_identifier_index</td>\n",
       "      <td>21672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Variable  Weight\n",
       "0                               tavg   60316\n",
       "1                        day_of_week   59485\n",
       "2                      month_of_data   56036\n",
       "3                        day_of_data   54591\n",
       "4             station_location_index   49466\n",
       "5                 lrs_location_point   38475\n",
       "6   posted_signed_route_number_index   36913\n",
       "7                   fips_county_code   36597\n",
       "8                     lane_of_travel   33319\n",
       "9                direction_of_travel   32968\n",
       "10          year_station_established   29943\n",
       "11          lrs_identification_index   27378\n",
       "12                      Weekend_Flag   25282\n",
       "13    direction_of_travel_name_index   23718\n",
       "14      hpms_sample_identifier_index   21672"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Printing the top 15 most important features\"\"\"\n",
    "\n",
    "Variable_Importance=pd.DataFrame(xgb_model.get_score(importance_type='weight').items())\n",
    "Variable_Importance.columns=['Variable','Weight']\n",
    "Variable_Importance=Variable_Importance.sort_values('Weight',ascending=[0]).reset_index(drop=True)\n",
    "Variable_Importance.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Exporting the Variable Importance\"\"\"\n",
    "\n",
    "writer=pd.ExcelWriter('.\\Output\\Variable_Importance.xlsx', engine='xlsxwriter')\n",
    "Variable_Importance.to_excel(writer, sheet_name='Variable_Importance', index=False)\n",
    "for index, var in enumerate(Variable_Importance):\n",
    "    max_length=max(Variable_Importance[var].astype(str).map(len).max(), len(var))+1\n",
    "    writer.sheets['Variable_Importance'].set_column(index, index, max_length)    \n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
